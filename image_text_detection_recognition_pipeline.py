import os
import glob
import json
import argparse
import sys
import yaml, numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from types import SimpleNamespace

# ë¡œì»¬ YOLOv5 ë° DTRB ë¦¬í¬ì§€í† ë¦¬ ê²½ë¡œ ì¶”ê°€
base_dir = os.path.dirname(os.path.abspath(__file__))
yolov5_dir = 'yolov5'
dtrb_dir = 'dtrb'
sys.path.insert(0, yolov5_dir)
sys.path.insert(0, dtrb_dir)
# forked repo ì²´í¬ ë¹„í™œì„±í™”
import torch
torch.hub._validate_not_a_forked_repo = lambda a, b, c: True

import cv2
from PIL import Image

# DTRB import (ë¡œì»¬) from deep-text-recognition-benchmark-master
from dtrb.utils import CTCLabelConverter, AttnLabelConverter
from dtrb.dataset import AlignCollate
from dtrb.model import Model

# Recognitionì— í•„ìš”í•œ charset loader í•¨ìˆ˜

def load_charset(path_or_str):
    """
       path_or_str: íŒŒì¼ ê²½ë¡œ, ë¬¸ìì—´, ë¦¬ìŠ¤íŠ¸ í—ˆìš©
       íŒŒì¼ ê²½ë¡œì¼ ê²½ìš° íŒŒì¼ì—ì„œ ì½ì–´ ë¬¸ì ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
       ë¬¸ìì—´ì¼ ê²½ìš° ê° ë¬¸ìë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
       ë¦¬ìŠ¤íŠ¸/íŠœí”Œì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
       """
    if isinstance(path_or_str, (list, tuple)):
        return list(path_or_str)
    try:
        if os.path.isfile(path_or_str):
            with open(path_or_str, 'r', encoding='utf-8') as f:
                chars = f.read().strip()
            return list(chars)
    except Exception:
        pass
    return list(str(path_or_str))


def load_recognition_model(args, device):
    # 1) ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (prefix stripping ì „)
    raw_state = torch.load(args.rec_weights, map_location=device)
    # 2) ì²« ë²ˆì§¸ conv weight key ì°¾ê¸°
    ckpt_key = next(k for k in raw_state if 'conv0_1.weight' in k)
    ckpt_in_ch = raw_state[ckpt_key].shape[1]  # 1 í˜¹ì€ 3
    args.input_channel = ckpt_in_ch

    # 3) ë¼ë²¨ ì»¨ë²„í„° ì´ˆê¸°í™”
    if 'CTC' in args.Prediction:
        converter = CTCLabelConverter(args.character)
    else:
        converter = AttnLabelConverter(args.character)
    args.num_class = len(converter.character)

    # 4) ëª¨ë¸ ìƒì„±
    rec_model = Model(args)

    # 5) module. prefix ì œê±° & ë¡œë“œ
    from collections import OrderedDict
    new_state = OrderedDict()
    for k, v in raw_state.items():
        name = k[7:] if k.startswith('module.') else k
        new_state[name] = v
    rec_model.load_state_dict(new_state)

    # 6) DataParallel wrapping
    rec_model = torch.nn.DataParallel(rec_model).to(device).eval()
    # rec_model.half()

    # 7) Transform ì„¸íŒ…
    transform = AlignCollate(imgH=args.imgH, imgW=args.imgW, keep_ratio_with_pad=args.PAD)
    return rec_model, converter, transform



def recognize_image(crop_pil, rec_model, converter, transform, args, device):
    # ëª¨ë¸ í•™ìŠµ ì‹œ rgb ì˜µì…˜ì— ë§ì¶° ì±„ë„ ë³€í™˜
    if args.rgb:
        crop_pil = crop_pil.convert('RGB')
    else:
        crop_pil = crop_pil.convert('L')  # grayscale

    # transform expects batch of (image, label)
    tensor, _ = transform([(crop_pil, '')])
    image = tensor.to(device).float()
    bs = image.size(0)
    text_input = torch.LongTensor(bs, args.batch_max_length + 1).fill_(0).to(device)

    with torch.no_grad():
        if 'CTC' in args.Prediction:
            preds = rec_model(image, text_input)
            sizes = torch.IntTensor([preds.size(1)] * bs)
            _, idx = preds.max(2)
            texts = converter.decode(idx, sizes)
            probs = preds.log_softmax(2).exp()
            max_probs, _ = probs.max(dim=2)
            confs = [p.cumprod(dim=0)[-1].item() for p in max_probs]
        else:
            preds = rec_model(image, text_input, is_train=False)
            _, idx = preds.max(2)
            texts = converter.decode(idx, torch.IntTensor([args.batch_max_length] * bs))
            text = texts[0]
            if '[s]' in text:
                eos = text.find('[s]')
                text = text[:eos]

            probs = torch.softmax(preds, dim=2)  # [bs, T, C]
            max_probs, _ = probs.max(dim=2)  # [bs, T]
            cumprod = max_probs.cumprod(dim=1)  # [bs, T] â† dim=1: ì‹œê°„ì¶•
            conf = cumprod[0, -1].item()  # ì²« ë°°ì¹˜(first sample)ì˜ ë§ˆì§€ë§‰ time-step

    return text, conf


def process_image_with_precomputed(image_path, detections_line: torch.Tensor,
                                   char_model, rec_model, converter, transform,
                                   args, device):
    capture_id = os.path.splitext(os.path.basename(image_path))[0]
    img_bgr = cv2.imread(image_path)
    if img_bgr is None:
        return []
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    H, W = img_rgb.shape[:2]
    pad = 5
    records = []

    # 1) ëª¨ë“  ë¼ì¸ í¬ë¡­ ë°°ì¹˜
    line_crops = []
    line_meta  = []
    for line_idx, det in enumerate(detections_line.tolist()):
        x1, y1, x2, y2, det_conf, _ = det
        if det_conf < args.line_conf:
            continue
        px1, py1 = max(int(x1)-pad,0), max(int(y1)-pad,0)
        px2, py2 = min(int(x2)+pad,W), min(int(y2)+pad,H)
        line_crops.append(img_rgb[py1:py2, px1:px2])
        line_meta.append((line_idx, px1, py1))

    if not line_crops:
        return []

    # 2) char detection ë°°ì¹˜
    char_results = char_model(line_crops)            # Detections ë¦¬ìŠ¤íŠ¸
    # 3) ëª¨ë“  ë¬¸ì í¬ë¡­ ì¤€ë¹„
    char_crops = []
    char_meta  = []
    for (line_idx, px1, py1), char_det in zip(line_meta, char_results.xyxy):
        for char_idx, cd in enumerate(char_det.tolist()):
            cx1, cy1, cx2, cy2, cconf, _ = cd
            if cconf < args.char_conf or cconf < 0.4:
                continue
            ax1, ay1 = px1+int(cx1), py1+int(cy1)
            ax2, ay2 = px1+int(cx2), py1+int(cy2)
            # â†’ PILë¡œ ë³€í™˜ í›„, ê·¸ë ˆì´ìŠ¤ì¼€ì¼(1ì±„ë„) í˜¹ì€ RGB(3ì±„ë„)ë¡œ ë§ì¶°ì¤Œ
            crop = Image.fromarray(img_rgb[ay1:ay2, ax1:ax2])
            if args.rgb:
                crop = crop.convert('RGB')
            else:
                crop = crop.convert('L')
            char_crops.append(crop)
            char_meta.append((line_idx, char_idx, ax1, ay1))

    # 4) recognition ë°°ì¹˜
    texts = []
    if char_crops:
        batch = [(crop, '') for crop in char_crops]
        tensor, _ = transform(batch)             # [N, C, H, W]
        image_batch = tensor.to(device).float()
        bs = image_batch.size(0)
        text_input = torch.LongTensor(bs, args.batch_max_length+1).fill_(0).to(device)

        from torch.amp import autocast
        with autocast(device_type='cuda'):
            if 'CTC' in args.Prediction:
                preds = rec_model(image_batch, text_input)
                sizes = torch.IntTensor([preds.size(1)]*bs)
                _, idx = preds.max(2)
                texts = converter.decode(idx, sizes)
            else:
                preds = rec_model(image_batch, text_input, is_train=False)
                _, idx = preds.max(2)
                texts = converter.decode(idx, torch.IntTensor([args.batch_max_length]*bs))
                # '[s]' ì´í›„ ì œê±°
                texts = [t.split('[s]')[0] if '[s]' in t else t for t in texts]

    # 5) ê²°ê³¼ ì¡°ë¦½
    for (line_idx, char_idx, ax1, ay1), text in zip(char_meta, texts):
        records.append({
            'capture': capture_id,
            'line':    f'L{line_idx}',
            'char':    f'C{char_idx}',
            'bbox':    [ax1, ay1, ax1, ay1],  # bboxë¥¼ ì •í™•íˆ ì“°ê³  ì‹¶ìœ¼ë©´ ë³„ë„ ì €ì¥
            'text':    text
        })

    return records

def inject_anchors_from_yaml(model, yolov5_dir, cfg_name, device):
    """
    model      : torch.hub.load(..., autoshape=False) ìœ¼ë¡œ ë°›ì€ DetectionModel
    yolov5_dir : yolov5 ì½”ë“œ í´ë” ê²½ë¡œ
    cfg_name   : 'yolov5s' ë“± (models/ ë°‘ì— ìˆëŠ” yaml ì´ë¦„, í™•ì¥ì ì œì™¸)
    device     : torch.device
    """
    # 1) yaml ì½ê¸° (UTF-8ë¡œ)
    cfg_path = os.path.join(yolov5_dir, 'models', f'{cfg_name}.yaml')
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)

    # 2) anchors ë°°ì—´ êº¼ë‚´ê¸°: ë¦¬ìŠ¤íŠ¸ of [xa,ya,xb,yb,...] í˜•íƒœ
    arr = np.array(cfg['anchors'], dtype=np.float32)  # shape (nl, na*2)
    # 3) Detect ë ˆì´ì–´ ë‚´ë¶€ì—ì„œ ì•µì»¤ë¥¼ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆ ì°¾ê¸°
    detect_layer = None
    for m in model.modules():
        if hasattr(m, 'anchors') and hasattr(m, 'anchor_grid'):
            detect_layer = m
            break
    if detect_layer is None:
        print("âš ï¸ Detect ë ˆì´ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•µì»¤ ë³€ê²½ ìƒëµ.")
        return

    # 4) tensor ë¡œ ë³€í™˜í•˜ì—¬ anchors ì†ì„±ë§Œ êµì²´
    nl = detect_layer.nl  # detection ë ˆì´ì–´ ìˆ˜
    na = detect_layer.na  # ì•µì»¤ per ë ˆì´ì–´
    new_anchors = arr.reshape(nl, na, 2)
    anchors_t = torch.from_numpy(new_anchors).to(device)
    detect_layer.anchors = anchors_t

    print(f"ğŸ”§ {cfg_name}.yaml ì•µì»¤({nl}Ã—{na})ë§Œ ì£¼ì… ì™„ë£Œ")
    # (anchor_grid ë²„í¼ëŠ” ê±´ë“œë¦¬ì§€ ì•Šì•„ë„ ë‹¤ìŒ forward ì‹œ _make_gridë¡œ ì¬ìƒì„±ë©ë‹ˆë‹¤)


from difflib import SequenceMatcher

def remove_redundant_lines(records, thresh=0.9):
    # 1) capture ì´ë¦„ ìˆœ ì •ë ¬ (HH-MM-SS í˜•ì‹ì´ë¼ ì‚¬ì „ìˆœ ì •ë ¬ë¡œ OK)
    caps = sorted({r['capture'] for r in records})
    cleaned = []
    prev_texts = None

    for cap in caps:
        # 2) í•œ ìº¡ì²˜ ì•ˆì˜ ë¼ì¸ë“¤ L0, L1â€¦ ìˆœì„œëŒ€ë¡œ ë½‘ê¸°
        lines = sorted(
            [r for r in records if r['capture']==cap],
            key=lambda r: int(r['line'][1:])
        )
        if prev_texts is None:
            # ì²« ìº¡ì²˜ëŠ” ê·¸ëŒ€ë¡œ
            cleaned.extend(lines)
        else:
            # 3) ë’¤ì—ì„œë¶€í„° ëŒë©´ì„œ ì´ì „ ìº¡ì²˜ í…ìŠ¤íŠ¸ì™€ ìœ ì‚¬ë„ ê²€ì‚¬
            drop_idx = None
            for i in range(len(lines)-1, -1, -1):
                for pt in prev_texts:
                    ratio = SequenceMatcher(None, lines[i]['text'], pt).ratio()
                    if ratio >= thresh:
                        drop_idx = i
                        break
                if drop_idx is not None:
                    break

            # 4) ì¤‘ë³µëœ ë¼ì¸ê³¼ ê·¸ ì• ë¼ì¸ ëª¨ë‘ ë²„ë¦¬ê³ , ê·¸ ë’¤ ë¼ì¸ë§Œ ì¶”ê°€
            if drop_idx is None:
                cleaned.extend(lines)
            else:
                cleaned.extend(lines[drop_idx+1:])

        # 5) ì´ë²ˆ ìº¡ì²˜ì˜ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ prev_textsë¡œ ê°±ì‹ 
        prev_texts = [r['text'] for r in lines]

    return cleaned


def run_pipeline(capture_dir,output_pipeline):
    defaults = {
        'rgb': False,
        'line_det_weights': 'yolov5/runs/train/detect_line3/weights/best.pt',
        'char_det_weights': 'yolov5/runs/train/detect_text/weights/best.pt',
        'rec_weights': 'dtrb/saved_models/text_nn_rec/best_accuracy.pth',
        'character': 'dtrb/charset.txt', 'input_channel': 1, 'output_channel':512, 'hidden_size':256,
        'imgH': 32, 'imgW': 100, 'batch_max_length': 25,
        'PAD': True, 'line_conf': 0.25, 'char_conf': 0.25,
        'workers': 4, 'device': 'cuda:0', 'batch_size': 8,
        'Prediction': 'Attn', 'Transformation':'None', 'FeatureExtraction':'ResNet', 'SequenceModeling':'None', 'num_fiducial':20,
    }
    defaults['character'] = ''.join(load_charset(defaults['character']))

    args_dict = defaults.copy()
    args_dict['capture_dir'] = capture_dir
    args_dict['output'] = output_pipeline
    args = SimpleNamespace(**args_dict)

    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')

    # Detection ëª¨ë¸ ë¡œë“œ (torch.hub local)
    # 1) ë¼ì¸ ê²€ì¶œ ëª¨ë¸
    line_det = torch.hub.load(yolov5_dir, 'custom', path=args.line_det_weights, source='local')
    line_det.to(device).eval()
    line_det.half()
    inject_anchors_from_yaml(line_det, yolov5_dir, 'yolov5s', device)

    # 2) ë¬¸ì(ê¸€ì) ê²€ì¶œ ëª¨ë¸
    char_det = torch.hub.load(yolov5_dir, 'custom', path=args.char_det_weights, source='local')
    char_det.to(device).eval()
    char_det.half()
    inject_anchors_from_yaml(char_det, yolov5_dir, 'yolov5s', device)


    rec_model, converter, transform = load_recognition_model(args, device)

    image_paths = sorted(glob.glob(os.path.join(args.capture_dir, '*.*')))
    all_records = []

    BATCH_SIZE = 8
    pbar = tqdm(total=len(image_paths), desc="Images processed")
    from torch.amp import autocast

    for i in range(0, len(image_paths), BATCH_SIZE):
        batch_paths = image_paths[i:i + BATCH_SIZE]
        imgs = [cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)
                for p in batch_paths]

        with autocast(device_type='cuda'):
            batch_results = line_det(imgs)

        # batch_results.xyxy: list of [n_det,6] tensors
        for img_path, det_tensor in zip(batch_paths, batch_results.xyxy):
            recs = process_image_with_precomputed(
                img_path, det_tensor, char_det,
                rec_model, converter, transform,
                args, device
            )
            all_records.extend(recs)
            pbar.update(1)
    pbar.close()

    from collections import defaultdict
    output_data = []

    # 1) captureë³„ë¡œ ë ˆì½”ë“œ ë¬¶ê¸°
    records_by_capture = defaultdict(list)
    for rec in all_records:
        records_by_capture[rec['capture']].append(rec)

    # 2) ê° captureì— ëŒ€í•´
    for cap, recs in sorted(records_by_capture.items(), key=lambda x: int(x[0]) if x[0].isdigit() else x[0]):
        # 2-1) ì›ë˜ ë¼ë²¨(L0, L1â€¦)ë³„ë¡œ ë¬¶ê¸°
        lines = defaultdict(list)
        for r in recs:
            lines[r['line']].append(r)

        # 2-2) ë¼ì¸ ê·¸ë£¹ì„ yì¶•(ìµœì†Œ y1) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_lines = sorted(
            lines.items(),
            key=lambda item: min(rec['bbox'][1] for rec in item[1])
        )

        # 2-3) ì •ë ¬ëœ ë¼ì¸ ìˆœì„œëŒ€ë¡œ, ê° ë¼ì¸ ë‚´ ë¬¸ìë“¤ì„ xì¶• ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        for line_label, chars in sorted_lines:
            chars.sort(key=lambda r: r['bbox'][0])
            line_text = ''.join(r['text'] for r in chars)

            output_data.append({
                'capture': cap,
                'line': line_label,
                'text': line_text
            })

    filtered = remove_redundant_lines(output_data, thresh=0.9)


    # 3) JSON ì €ì¥
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(filtered, f, ensure_ascii=False, indent=2)
    print(f'Results saved to {args.output}')


if __name__ == '__main__':
    run_pipeline('frame','pipeline_result.json')
