import os
import glob
import json
import sys
import torch
import yaml
import numpy as np
from tqdm import tqdm
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from types import SimpleNamespace

# ë¡œì»¬ YOLOv5 ë° DTRB ë¦¬í¬ì§€í† ë¦¬ ê²½ë¡œ ì¶”ê°€
base_dir = os.path.dirname(os.path.abspath(__file__))
yolov5_dir = 'yolov5'
dtrb_dir = 'dtrb'
sys.path.insert(0, yolov5_dir)
sys.path.insert(0, dtrb_dir)
# forked repo ì²´í¬ ë¹„í™œì„±í™”
import torch
torch.hub._validate_not_a_forked_repo = lambda a, b, c: True

import cv2
from PIL import Image
# DTRB import (ë¡œì»¬) from deep-text-recognition-benchmark-master
from dtrb.utils import CTCLabelConverter, AttnLabelConverter
from dtrb.dataset import AlignCollate
from dtrb.model import Model

# -- (ê¸°ì¡´ import ìƒëµ) --
def load_charset(path_or_str):
    """
       path_or_str: íŒŒì¼ ê²½ë¡œ, ë¬¸ìì—´, ë¦¬ìŠ¤íŠ¸ í—ˆìš©
       íŒŒì¼ ê²½ë¡œì¼ ê²½ìš° íŒŒì¼ì—ì„œ ì½ì–´ ë¬¸ì ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
       ë¬¸ìì—´ì¼ ê²½ìš° ê° ë¬¸ìë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
       ë¦¬ìŠ¤íŠ¸/íŠœí”Œì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
       """
    if isinstance(path_or_str, (list, tuple)):
        return list(path_or_str)
    try:
        if os.path.isfile(path_or_str):
            with open(path_or_str, 'r', encoding='utf-8') as f:
                chars = f.read().strip()
            return list(chars)
    except Exception:
        pass
    return list(str(path_or_str))

def load_recognition_model(args, device):
    # 1) ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (prefix stripping ì „)
    raw_state = torch.load(args.rec_weights, map_location=device)
    # 2) ì²« ë²ˆì§¸ conv weight key ì°¾ê¸°
    ckpt_key = next(k for k in raw_state if 'conv0_1.weight' in k)
    ckpt_in_ch = raw_state[ckpt_key].shape[1]  # 1 í˜¹ì€ 3
    args.input_channel = ckpt_in_ch

    # 3) ë¼ë²¨ ì»¨ë²„í„° ì´ˆê¸°í™”
    if 'CTC' in args.Prediction:
        converter = CTCLabelConverter(args.character)
    else:
        converter = AttnLabelConverter(args.character)
    args.num_class = len(converter.character)

    # 4) ëª¨ë¸ ìƒì„±
    rec_model = Model(args)

    # 5) module. prefix ì œê±° & ë¡œë“œ
    from collections import OrderedDict
    new_state = OrderedDict()
    for k, v in raw_state.items():
        name = k[7:] if k.startswith('module.') else k
        new_state[name] = v
    rec_model.load_state_dict(new_state)

    # 6) DataParallel wrapping
    rec_model = torch.nn.DataParallel(rec_model).to(device).eval()
    # rec_model.half()

    # 7) Transform ì„¸íŒ…
    transform = AlignCollate(imgH=args.imgH, imgW=args.imgW, keep_ratio_with_pad=args.PAD)
    return rec_model, converter, transform

def inject_anchors_from_yaml(model, yolov5_dir, cfg_name, device):
    """
    model      : torch.hub.load(..., autoshape=False) ìœ¼ë¡œ ë°›ì€ DetectionModel
    yolov5_dir : yolov5 ì½”ë“œ í´ë” ê²½ë¡œ
    cfg_name   : 'yolov5s' ë“± (models/ ë°‘ì— ìˆëŠ” yaml ì´ë¦„, í™•ì¥ì ì œì™¸)
    device     : torch.device
    """
    # 1) yaml ì½ê¸° (UTF-8ë¡œ)
    cfg_path = os.path.join(yolov5_dir, 'models', f'{cfg_name}.yaml')
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)

    # 2) anchors ë°°ì—´ êº¼ë‚´ê¸°: ë¦¬ìŠ¤íŠ¸ of [xa,ya,xb,yb,...] í˜•íƒœ
    arr = np.array(cfg['anchors'], dtype=np.float32)  # shape (nl, na*2)
    # 3) Detect ë ˆì´ì–´ ë‚´ë¶€ì—ì„œ ì•µì»¤ë¥¼ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆ ì°¾ê¸°
    detect_layer = None
    for m in model.modules():
        if hasattr(m, 'anchors') and hasattr(m, 'anchor_grid'):
            detect_layer = m
            break
    if detect_layer is None:
        print("âš ï¸ Detect ë ˆì´ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•µì»¤ ë³€ê²½ ìƒëµ.")
        return

    # 4) tensor ë¡œ ë³€í™˜í•˜ì—¬ anchors ì†ì„±ë§Œ êµì²´
    nl = detect_layer.nl  # detection ë ˆì´ì–´ ìˆ˜
    na = detect_layer.na  # ì•µì»¤ per ë ˆì´ì–´
    new_anchors = arr.reshape(nl, na, 2)
    anchors_t = torch.from_numpy(new_anchors).to(device)
    detect_layer.anchors = anchors_t

    print(f"ğŸ”§ {cfg_name}.yaml ì•µì»¤({nl}Ã—{na})ë§Œ ì£¼ì… ì™„ë£Œ")
    # (anchor_grid ë²„í¼ëŠ” ê±´ë“œë¦¬ì§€ ì•Šì•„ë„ ë‹¤ìŒ forward ì‹œ _make_gridë¡œ ì¬ìƒì„±ë©ë‹ˆë‹¤)

def group_chars_into_lines(records, y_thresh=20):
    """
    records: [{'bbox':[x1,y1,x2,y2], 'text':str, 'capture':id}, â€¦]
    y_thresh: ê°™ì€ ì¤„ë¡œ ë³¼ ìµœëŒ€ y ì¤‘ì‹¬ ì°¨ì´(px)
    """
    # 1) ê° ë¬¸ì ë ˆì½”ë“œì— center_y ì¶”ê°€
    items = [
        {'rec': r,
         'cy': (r['bbox'][1] + r['bbox'][3]) / 2}
        for r in records
    ]
    # 2) y ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
    items.sort(key=lambda x: x['cy'])

    lines = []
    for it in items:
        placed = False
        for line in lines:
            # ê¸°ì¡´ ë¼ì¸ì˜ í‰ê·  center_y ì™€ ë¹„êµ
            if abs(it['cy'] - line['mean_cy']) <= y_thresh:
                line['recs'].append(it['rec'])
                # í‰ê·  y ì—…ë°ì´íŠ¸
                cy_list = [(r['bbox'][1] + r['bbox'][3]) / 2 for r in line['recs']]
                line['mean_cy'] = sum(cy_list) / len(cy_list)
                placed = True
                break
        if not placed:
            lines.append({
                'mean_cy': it['cy'],
                'recs': [it['rec']]
            })
    # 3) ê° ì¤„ë³„ë¡œ x ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ëœ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜
    grouped = []
    for line in lines:
        recs = sorted(line['recs'], key=lambda r: r['bbox'][0])
        grouped.append(recs)
    return grouped

from difflib import SequenceMatcher

def remove_redundant_lines(records, thresh=0.9):
    # 1) capture ì´ë¦„ ìˆœ ì •ë ¬ (HH-MM-SS í˜•ì‹ì´ë¼ ì‚¬ì „ìˆœ ì •ë ¬ë¡œ OK)
    caps = sorted({r['capture'] for r in records})
    cleaned = []
    prev_texts = None

    for cap in caps:
        # 2) í•œ ìº¡ì²˜ ì•ˆì˜ ë¼ì¸ë“¤ L0, L1â€¦ ìˆœì„œëŒ€ë¡œ ë½‘ê¸°
        lines = sorted(
            [r for r in records if r['capture']==cap],
            key=lambda r: int(r['line'][1:])
        )
        if prev_texts is None:
            # ì²« ìº¡ì²˜ëŠ” ê·¸ëŒ€ë¡œ
            cleaned.extend(lines)
        else:
            # 3) ë’¤ì—ì„œë¶€í„° ëŒë©´ì„œ ì´ì „ ìº¡ì²˜ í…ìŠ¤íŠ¸ì™€ ìœ ì‚¬ë„ ê²€ì‚¬
            drop_idx = None
            for i in range(len(lines)-1, -1, -1):
                for pt in prev_texts:
                    ratio = SequenceMatcher(None, lines[i]['text'], pt).ratio()
                    if ratio >= thresh:
                        drop_idx = i
                        break
                if drop_idx is not None:
                    break

            # 4) ì¤‘ë³µëœ ë¼ì¸ê³¼ ê·¸ ì• ë¼ì¸ ëª¨ë‘ ë²„ë¦¬ê³ , ê·¸ ë’¤ ë¼ì¸ë§Œ ì¶”ê°€
            if drop_idx is None:
                cleaned.extend(lines)
            else:
                cleaned.extend(lines[drop_idx+1:])

        # 5) ì´ë²ˆ ìº¡ì²˜ì˜ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ prev_textsë¡œ ê°±ì‹ 
        prev_texts = [r['text'] for r in lines]

    return cleaned

def run_pipeline(capture_dir, output_path):
    # -- default args ì„¸íŒ… (ê¸°ì¡´ê³¼ ë™ì¼) --
    defaults = {
        'rgb': False,
        'char_det_weights': 'yolov5/runs/train/detect_text/weights/best.pt',
        'rec_weights': 'dtrb/saved_models/text_nn_rec/best_accuracy.pth',
        'character': 'dtrb/charset.txt', 'input_channel': 1, 'output_channel':512, 'hidden_size':256,
        'imgH': 32, 'imgW': 100, 'batch_max_length': 25,
        'PAD': True, 'char_conf': 0.25,
        'workers': 4, 'device': 'cuda:0', 'batch_size': 8,
        'Prediction': 'Attn', 'Transformation':'None', 'FeatureExtraction':'ResNet', 'SequenceModeling':'None', 'num_fiducial':20,
    }
    defaults['character'] = ''.join(load_charset(defaults['character']))
    args = SimpleNamespace(**{**defaults,
                             'capture_dir': capture_dir,
                             'output': output_path})

    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')

    # ë¬¸ì ê²€ì¶œ ëª¨ë¸ í•œ ê°œë§Œ ë¡œë“œ
    text_det = torch.hub.load('yolov5', 'custom',
                              path=args.char_det_weights,
                              source='local').to(device).eval()
    text_det.half()

    rec_model, converter, transform = load_recognition_model(args, device)

    image_paths = sorted(glob.glob(os.path.join(capture_dir, '*.*')))
    all_outputs = []

    pbar = tqdm(total=len(image_paths), desc="Images processed")
    from torch.amp import autocast

    for img_path in image_paths:
        img_bgr = cv2.imread(img_path)
        if img_bgr is None:
            pbar.update(1)
            continue
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

        # 1) ë¬¸ì ê²€ì¶œ
        with autocast(device_type='cuda'):
            results = text_det([img_rgb])
        dets = results.xyxy[0].cpu().numpy()  # [ [x1,y1,x2,y2,conf,cls], â€¦ ]

        # 2) ê° ë¬¸ìì— ëŒ€í•´ ì¸ì‹
        records = []
        capture_id = os.path.splitext(os.path.basename(img_path))[0]
        for x1, y1, x2, y2, conf, _ in dets:
            if conf < args.char_conf:
                continue

            crop = Image.fromarray(
                img_rgb[int(y1):int(y2), int(x1):int(x2)]
            )
            crop = crop.convert('RGB' if args.rgb else 'L')

            # recognition
            tensor, _ = transform([(crop, '')])
            image_batch = tensor.to(device).float()
            bs = image_batch.size(0)
            text_input = torch.LongTensor(
                bs, args.batch_max_length + 1
            ).fill_(0).to(device)

            with autocast(device_type='cuda'):
                if 'CTC' in args.Prediction:
                    preds = rec_model(image_batch, text_input)
                    sizes = torch.IntTensor([preds.size(1)] * bs)
                    _, idx = preds.max(2)
                    texts = converter.decode(idx, sizes)
                    text = texts[0]
                else:
                    preds = rec_model(image_batch, text_input, is_train=False)
                    _, idx = preds.max(2)
                    texts = converter.decode(idx,
                                             torch.IntTensor([args.batch_max_length] * bs))
                    text = texts[0].split('[s]')[0]

            records.append({
                'capture': capture_id,
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'text': text
            })
        #bbox ìœ„ì¹˜ ë””ë²„ê·¸
        #print("â–¶ raw records:", records)
        # 3) yì¶• ê¸°ì¤€ìœ¼ë¡œ ë¼ì¸ ê·¸ë£¹í•‘ â†’ ì¤„ë³„ í…ìŠ¤íŠ¸ ì¡°í•©
        lines = group_chars_into_lines(records, y_thresh=30)
        #print("â–¶ grouped lines:", lines)
        for i, line_recs in enumerate(lines):
            #x1_min = min(r['bbox'][0] for r in line_recs)
            #y1_min = min(r['bbox'][1] for r in line_recs)
            #x2_max = max(r['bbox'][2] for r in line_recs)
            #y2_max = max(r['bbox'][3] for r in line_recs)
            line_text = ''.join(r['text'] for r in line_recs)
            all_outputs.append({
                'capture': capture_id,
                'line': f'L{i}',
                'text': line_text,
                #'bbox': [x1_min, y1_min, x2_max, y2_max]
            })

        pbar.update(1)

    filtered = remove_redundant_lines(all_outputs, thresh=0.8)
    pbar.close()

    # 4) ê²°ê³¼ JSON ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(filtered, f, ensure_ascii=False, indent=2)
    print(f"Results saved to {output_path}")

if __name__ == '__main__':
    run_pipeline('exex', 'pipeline_result1.json')



